{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import log_loss, make_scorer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.metrics import BinaryCrossentropy\n",
    "from methods import *\n",
    "import torch\n",
    "import pickle, os\n",
    "import skopt\n",
    "from skopt import BayesSearchCV, gp_minimize\n",
    "from skopt.space import Integer\n",
    "from skopt.space import Real\n",
    "from skopt.space import Categorical\n",
    "from skopt.utils import use_named_args\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_csv = fr'confusion-reports\\ics_fc\\round3-train-dataset\\round3-train-dataset_fc_square-30-random_filters.csv'\n",
    "df = pd.read_csv(path_csv)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = read_features(path_csv, trigger_type_aux_str=None, arch=None, data='diffs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((1008, 9))\n",
    "b = np.zeros((1008, 12))\n",
    "c = np.hstack((a,b))\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_csv = fr'confusion-reports\\ics_svm\\round3-train-dataset\\round3-train-dataset_square-30-filters_all-classes_gray.csv'\n",
    "# path_csv = fr'confusion-reports\\ics_fc\\round3-train-dataset\\round3-train-dataset_fc_square-30-gray_filters.csv'\n",
    "path_csv = fr'confusion-reports\\ics_svm\\round2-holdout-dataset\\round2-holdout-dataset_square-25-filters_all-classes_gray_confusion-matrix.csv'\n",
    "df = pd.read_csv(path_csv)\n",
    "df['model_label'] = 1 - df['model_label'].astype('category').cat.codes\n",
    "del df['num_classes']\n",
    "indexes = []\n",
    "arch = None\n",
    "if arch is not None:\n",
    "    for i in range(len(df)):\n",
    "        if df['model_architecture'].iloc[i].startswith(arch):\n",
    "            indexes.append(i)\n",
    "    df = df.iloc[indexes]\n",
    "for c in df.columns:\n",
    "    if c.endswith('mean') or c.endswith('std') or c.startswith('kl'):\n",
    "#     if c.endswith('diff'):\n",
    "        del df[c]\n",
    "print(df.shape)\n",
    "print(df.columns.tolist())\n",
    "plt.figure(figsize=(30,12)).patch.set_color('white')\n",
    "plt.title(arch)\n",
    "sns.heatmap(df.corr(), annot=True, cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box-plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_csv = fr'confusion-reports\\ics_svm\\round3-train-dataset\\round3-train-dataset_square-30-filters_all-classes_gray.csv'\n",
    "path_csv = fr'confusion-reports\\ics_fc\\round3-train-dataset\\round3-train-dataset_fc_square-30-gray_filters.csv'\n",
    "df = pd.read_csv(path_csv)\n",
    "df['model_label'] = 1 - df['model_label'].astype('category').cat.codes\n",
    "del df['num_classes'], df['model_name']\n",
    "indexes = []\n",
    "arch = None\n",
    "if arch is not None:\n",
    "    for i in range(len(df)):\n",
    "        if df['model_architecture'].iloc[i].startswith(arch):\n",
    "            indexes.append(i)\n",
    "    df = df.iloc[indexes]\n",
    "for c in df.columns:\n",
    "    if c.endswith('mean') or c.endswith('std'):\n",
    "#     if c.endswith('diff'):\n",
    "        del df[c]\n",
    "del df['model_architecture'], df['trigger_type_aux'], df['trigger_color'], df['model_label']\n",
    "print(df.columns.tolist())\n",
    "plt.figure(figsize=(30, 12)).patch.set_color('white')\n",
    "df.boxplot()\n",
    "# df.boxplot(by='model_label', figsize=(30,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_csv = fr'confusion-reports\\ics_svm\\round3-train-dataset\\round3-train-dataset_square-30-filters_all-classes_gray.csv'\n",
    "path_csv = fr'confusion-reports\\ics_fc\\round3-train-dataset\\round3-train-dataset_fc_square-30-gray_filters.csv'\n",
    "df = pd.read_csv(path_csv)\n",
    "df['model_label'] = 1 - df['model_label'].astype('category').cat.codes\n",
    "indexes = []\n",
    "arch = None\n",
    "if arch is not None:\n",
    "    for i in range(len(df)):\n",
    "        if df['model_architecture'].iloc[i].startswith(arch):\n",
    "            indexes.append(i)\n",
    "    df = df.iloc[indexes]\n",
    "for c in df.columns:\n",
    "    if c.endswith('mean') or c.endswith('std'):\n",
    "#     if c.endswith('diff'):\n",
    "        del df[c]\n",
    "del df['model_architecture'], df['trigger_type_aux'], df['trigger_color'], df['num_classes'], df['model_name']\n",
    "# print(df.head())\n",
    "\n",
    "data = df.values\n",
    "X, y = data[:, 1:], data[:, 0]\n",
    "iso = IsolationForest(contamination=0.1)\n",
    "yhat = iso.fit_predict(X)\n",
    "mask = np.where(yhat == 1)[0]\n",
    "print(mask.shape)\n",
    "del df['model_label']\n",
    "plt.figure(figsize=(30, 12)).patch.set_color('white')\n",
    "df.iloc[mask].boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratified K-fold validation for a full training dataset (square-size and 5 filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# path_csv = r'confusion-reports\\ics_svm\\round2-train-dataset\\round2-train-dataset_square25-filters_triggered_classes.csv' # old dataset\n",
    "# path_csv = r'confusion-reports\\ics_svm\\round2-train-dataset\\round2-train-dataset_square-25-filters_all-classes_gray.csv'\n",
    "# path_csv = r'confusion-reports\\ics_svm\\round2-train-dataset\\round2-train-dataset_square-25-filters_all-classes_gray_confusion-matrix.csv'\n",
    "n_splits = 10\n",
    "n_repeats = 3\n",
    "\n",
    "for size in [30]: #[10, 15, 20, 25, 30, 35, 40, 45, 50]:\n",
    "#     path_csv = fr'confusion-reports\\ics_svm\\round3-train-dataset\\round3-train-dataset_square-{size}-random-filters_all-classes_gray.csv'\n",
    "    path_csv = fr'confusion-reports\\ics_fc\\round3-train-dataset\\round3-train-dataset_fc_square-{size}-gray_filters.csv'\n",
    "    for A in [None]: #['vgg', 'resnet', 'wideresnet', 'densenet', 'mobilenet', 'shufflenet', 'squeezenet', 'inception', 'googlenet', None]:\n",
    "        X, y = read_features(path_csv, trigger_type_aux_str=None, arch=A, data='diffs')\n",
    "        print('arch=', A)\n",
    "        \n",
    "        scores_roc, scores_xent = [], []\n",
    "        for _ in range(n_repeats):\n",
    "            kfold =  StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "            for train_index, test_index in kfold.split(X, y):\n",
    "                X_train, y_train = X[train_index, :], y[train_index]\n",
    "                X_test, y_test = X[test_index, :], y[test_index]\n",
    "                scaler = StandardScaler()\n",
    "                scaler.fit(X_train)\n",
    "                X_train = scaler.transform(np.copy(X_train))\n",
    "                X_test = scaler.transform(np.copy(X_test))\n",
    "                roc, xent = evaluate_classifier(LogisticRegression(C=1), X_train, y_train, X_test, y_test)\n",
    "                scores_roc.append(roc)\n",
    "                scores_xent.append(xent)\n",
    "        print(f'size={size}')\n",
    "        print(f'CE mean={np.mean(scores_xent):.3f}, CE std={np.std(scores_xent):.3f}')\n",
    "#         print(f'ROC mean={np.mean(scores_roc):.3f}, ROC std={np.std(scores_roc):.3f}\\n')\n",
    "#         pipeline = Pipeline([('classifier', get_base_classifier())])\n",
    "#         kfold = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=666)\n",
    "#         results = cross_val_score(pipeline, X, y, cv=kfold, scoring='neg_log_loss', n_jobs=-1)\n",
    "#         print(f'size={size}: CE mean={-results.mean():.3f}, CE std={results.std():.3f}\\n')\n",
    "        print('-------------------------------------------------------------------------')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPO for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using diffs\n",
      "(1008, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>CE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.550343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.099</td>\n",
       "      <td>0.550450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.098</td>\n",
       "      <td>0.550560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.097</td>\n",
       "      <td>0.550672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.096</td>\n",
       "      <td>0.550787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.601005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.004</td>\n",
       "      <td>0.605269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003</td>\n",
       "      <td>0.611027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002</td>\n",
       "      <td>0.619812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.636659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        C        CE\n",
       "99  0.100  0.550343\n",
       "98  0.099  0.550450\n",
       "97  0.098  0.550560\n",
       "96  0.097  0.550672\n",
       "95  0.096  0.550787\n",
       "..    ...       ...\n",
       "4   0.005  0.601005\n",
       "3   0.004  0.605269\n",
       "2   0.003  0.611027\n",
       "1   0.002  0.619812\n",
       "0   0.001  0.636659\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path_csv = fr'confusion-reports\\ics_fc\\round3-train-dataset\\round3-train-dataset_fc_square-30-gray_filters.csv'\n",
    "# path_csv = fr'confusion-reports\\ics_fc\\round3-train-dataset\\round3-train-dataset_fc_square-30-random_filters.csv'\n",
    "path_csv = fr'confusion-reports\\ics_svm\\round3-train-dataset\\round3-train-dataset_square-30_filters.csv'\n",
    "# path_csv = fr'confusion-reports\\ics_svm\\round3-train-dataset\\round3-train-dataset_square-30-random_filters.csv'\n",
    "X, y = read_features(path_csv, trigger_type_aux_str=None, arch=None, data='diffs', append_arch=True)\n",
    "print(X.shape)\n",
    "params = {\n",
    "    \"C\": [0.001 + i * 0.001 for i in range(100)]\n",
    "}\n",
    "grid_cv = GridSearchCV(estimator=LogisticRegression(),\n",
    "                       param_grid=params,\n",
    "                       cv=RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1),\n",
    "                       scoring=make_scorer(log_loss, greater_is_better=False, needs_proba=True),\n",
    "                       n_jobs=-1)\n",
    "grid_cv.fit(X, y)\n",
    "\n",
    "# print('Best params:', grid_cv.best_params_)\n",
    "# print('Best CE:', grid_cv.best_score_)\n",
    "pd.DataFrame.from_dict({\n",
    "    'C': [d['C'] for d in grid_cv.cv_results_['params']],\n",
    "    'CE': -grid_cv.cv_results_['mean_test_score']\n",
    "}).sort_values(['CE'], ascending=True, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train meta-model using square data and filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using diffs\n",
      "(1008, 20)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# path_csv = fr'confusion-reports\\ics_fc\\round3-train-dataset\\round3-train-dataset_fc_square-30-random_filters.csv'\n",
    "path_csv = fr'confusion-reports\\ics_svm\\round3-train-dataset\\round3-train-dataset_square-30-random_filters.csv'\n",
    "\n",
    "X_train, y_train = read_features(path_csv, trigger_type_aux_str=None, arch=None, data='diffs', append_arch=True)\n",
    "# model = svm.SVC(C=11, kernel='rbf', gamma='scale', probability=True)\n",
    "print(X_train.shape)\n",
    "scaler=None\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train)\n",
    "# X_train = scaler.transform(X_train)\n",
    "\n",
    "model = LogisticRegression(C=2)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "name = '../metamodels/metamodel_12_svm_round3_data=diffs_square=30-rand_scaler=NO_clf=LR-2_arch-features=YES'\n",
    "if not os.path.isdir(name):\n",
    "    os.mkdir(name)\n",
    "save_obj(model, f'{name}/model.pickle')\n",
    "save_obj(scaler, f'{name}/scaler.pickle')\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_csv = fr'confusion-reports\\ics_fc\\round3-train-dataset\\round3-train-dataset_fc_square-30-gray_filters.csv'\n",
    "\n",
    "X, y = read_features(path_csv, trigger_type_aux_str=None, arch=None, data='diffs')\n",
    "X_train, y_train = X[:500, :], y[:500]\n",
    "X_test, y_test = X[500:, :], y[500:]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "model = LogisticRegression(C=1)\n",
    "model.fit(X_train, y_train)\n",
    "ce = 0.0\n",
    "for i in range(X_test.shape[0]):\n",
    "    prob = model.predict_proba(X_test[i,:].reshape(1, -1))\n",
    "    p0, p1 = prob[0]\n",
    "    t = y[i]\n",
    "    local_ce = -(t * np.log2(p1) + (1-t) * np.log2(1-p0))\n",
    "#     print(i, local_ce)\n",
    "    ce += local_ce\n",
    "print(ce / X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_neural_network():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim=14, activation='relu'))\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def cross_validation(data_x, data_y):\n",
    "#     pipeline = Pipeline([('scale', MinMaxScaler()), ('mlp', KerasClassifier(build_fn=create_neural_network, epochs=100, batch_size=32, verbose=0))])\n",
    "#     for _ in range(3):\n",
    "#         pipeline = Pipeline([('mlp', KerasClassifier(build_fn=create_neural_network, epochs=50, batch_size=16, verbose=0))])\n",
    "#         outer_kfold = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=666)\n",
    "#         for idx_train_val, idx_test in outer_kfold.split(data_x, data_y):\n",
    "#             subset_data_x = data_x[idx_train_val, :]\n",
    "#             subset_data_y = data_y[idx_train_val]\n",
    "#             inner_kfold = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=666)\n",
    "    pipeline = Pipeline([('mlp', KerasClassifier(build_fn=create_neural_network, epochs=400, batch_size=32, verbose=0))])\n",
    "    kfold = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=666)\n",
    "    results = cross_val_score(pipeline, data_x, data_y, cv=kfold, scoring='neg_log_loss', n_jobs=-1)\n",
    "    results = results[~np.isnan(results)]\n",
    "    results = results[~np.isinf(results)]\n",
    "    print(f'CV: CE mean={-results.mean():.3f} CE std={results.std():.3f}')\n",
    "    \n",
    "def train_nn(data_x, data_y, test_x=None, test_y=None):\n",
    "    model = create_neural_network()\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(data_x)\n",
    "    data_x_scaled = scaler.transform(data_x)\n",
    "    history = model.fit(data_x_scaled, data_y, epochs=100, batch_size=32, verbose=0)\n",
    "    \n",
    "    loss, accuracy = model.evaluate(data_x_scaled, data_y, verbose=0)\n",
    "    print(f'Train: loss={loss:.3f} accuracy={accuracy:.3f}')\n",
    "    \n",
    "    if test_x is not None and test_y is not None:\n",
    "        loss, accuracy = model.evaluate(test_x, test_y, verbose=0)\n",
    "        print(f'Test: loss={loss:.3f} accuracy={accuracy:.3f}')\n",
    "        \n",
    "#     folder = r'D:\\Cloud\\MEGA\\TrojAI\\TrojAI-UMD\\metamodels\\metamodel_10_round3_NN-60-30_min-max-scaled_RAW_square30_RANDOM_filters_all-classes'\n",
    "    folder = r'D:\\Cloud\\MEGA\\TrojAI\\TrojAI-UMD\\metamodels\\metamodel_11_round3_NN-60-30_min-max-scaled_RAW_square30_GRAY'\n",
    "    keras_save(model, folder)\n",
    "    save_obj(scaler, os.path.join(folder, 'scaler.pickle'))\n",
    "\n",
    "path_csv = r'confusion-reports\\ics_svm\\round3-train-dataset\\round3-train-dataset_square-30-filters_all-classes_gray.csv'\n",
    "X, y = read_features_raw(path_csv, None)\n",
    "\n",
    "# n = 800\n",
    "# X_train, y_train = X[:n, :], y[:n]\n",
    "# X_test, y_test = X[n:, :], y[n:]\n",
    "# print(X_train.shape, y_train.shape) #, X_test.shape, y_test.shape)\n",
    "\n",
    "\n",
    "# cross_validation(X, y)\n",
    "train_nn(X, y)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Testing: train on training data and test on holdout data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_csv = r'confusion-reports\\ics_svm\\round2-train-dataset\\round2-train-dataset_square-25-filters_all-classes_gray.csv'\n",
    "path_holdout_csv = r'confusion-reports\\ics_svm\\round2-holdout-dataset\\round2-holdout-dataset_square-25-filters_all-classes_gray.csv'\n",
    "# path_train_csv = r'confusion-reports\\ics_svm\\round2-train-dataset\\round2-train-dataset_square-25-filters_all-classes_gray_confusion-matrix.csv'\n",
    "# path_holdout_csv = r'confusion-reports\\ics_svm\\round2-holdout-dataset\\round2-holdout-dataset_square-25-filters_all-classes_gray_confusion-matrix.csv'\n",
    "trigger_type_aux_str = None\n",
    "\n",
    "print('Local Testing')\n",
    "if 'confusion-matrix' in path_train_csv and 'confusion-matrix' in path_holdout_csv:\n",
    "    print('Approach: confusion matrix and original CNN')\n",
    "    X_train, y_train = read_features_confusion_matrix(path_train_csv, trigger_type_aux_str)\n",
    "    X_holdout, y_holdout = read_features_confusion_matrix(path_holdout_csv, trigger_type_aux_str)\n",
    "else:\n",
    "    print('Approach: confusion distribution and SDNs')\n",
    "    X_train, y_train = read_features(path_train_csv, trigger_type_aux_str)\n",
    "    X_holdout, y_holdout = read_features(path_holdout_csv, trigger_type_aux_str)\n",
    "\n",
    "print('train shape:', X_train.shape, y_train.shape)\n",
    "print('holdout shape:', X_holdout.shape, y_holdout.shape)\n",
    "\n",
    "roc, xent = evaluate_classifier(X_train, y_train, X_holdout, y_holdout)\n",
    "print(f'ROC AUC = {roc}')\n",
    "print(f'Cross-Entropy = {xent}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
