{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import log_loss, make_scorer, accuracy_score, precision_score, recall_score, confusion_matrix, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import log_loss, make_scorer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.metrics import BinaryCrossentropy\n",
    "from methods import *\n",
    "import torch\n",
    "import pickle, os\n",
    "import skopt\n",
    "from skopt import BayesSearchCV, gp_minimize\n",
    "from skopt.space import Integer\n",
    "from skopt.space import Real\n",
    "from skopt.space import Categorical\n",
    "from skopt.utils import use_named_args\n",
    "import shutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using diffs\n",
      "Using diffs\n",
      "indexes_test: [3, 4, 5, 9, 10, 12, 16, 21, 30, 32, 38, 42, 43, 44, 46, 49, 50, 52, 53, 57, 58, 65, 66, 67, 71, 79, 83, 84, 87, 89, 97, 98, 105, 106, 108, 109, 113, 115, 116, 120, 124, 127, 132, 137, 140, 141, 142, 149, 150, 155, 178, 180, 182, 185, 186, 187, 189, 198, 199, 200, 205, 215, 217, 222, 226, 227, 233, 235, 237, 247, 251, 254, 255, 263, 265, 268, 277, 283, 285, 286, 289, 290, 293, 297, 298, 302, 304, 307, 310, 311, 312, 313, 315, 325, 327, 328, 341, 348, 350, 353, 354, 365, 366, 369, 371, 375, 379, 383, 384, 386, 387, 388, 392, 393, 395, 399, 401, 403, 404, 405, 408, 411, 415, 416, 417, 419, 420, 421, 426, 427, 428, 431, 434, 440, 443, 447, 448, 451, 453, 458, 460, 464, 465, 466, 468, 470, 471, 472, 478, 482, 485, 488, 490, 493, 494, 507, 511, 519, 520, 522, 525, 528, 530, 531, 534, 536, 539, 540, 542, 543, 544, 547, 549, 557, 565, 571, 580, 582, 583, 585, 592, 594, 595, 599, 604, 605, 608, 610, 612, 616, 622, 625, 629, 631, 632, 637, 640, 643, 647, 653, 654, 657, 659, 660, 664, 671, 672, 677, 684, 685, 686, 687, 690, 696, 700, 704, 705, 706, 713, 717, 725, 727, 732, 735, 739, 740, 741, 744, 749, 753, 754, 755, 760, 763, 765, 769, 770, 772, 774, 776, 778, 779, 783, 786, 790, 797, 805, 806, 810, 814, 815, 816, 824, 825, 831, 832, 833, 837, 841, 844, 845, 847, 848, 849, 850, 851, 855, 858, 859, 872, 875, 879, 880, 881, 884, 885, 886, 896, 899, 902, 905, 910, 912, 932, 933, 935, 936, 937, 940, 941, 943, 945, 951, 957, 961, 963, 965, 973, 979, 980, 987, 989, 1005]\n",
      "(705, 20) (705,) (303, 20) (303,)\n",
      "(705, 20) (705,) (303, 20) (303,)\n",
      "#################### BACKDOOR 0 ####################\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\Ionut-Vlad Modoranu\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\Ionut-Vlad Modoranu\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\Ionut-Vlad Modoranu\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\Ionut-Vlad Modoranu\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\Ionut-Vlad Modoranu\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\Ionut-Vlad Modoranu\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\Ionut-Vlad Modoranu\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:748 train_step\n        loss = self.compiled_loss(\n    C:\\Users\\Ionut-Vlad Modoranu\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:204 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    C:\\Users\\Ionut-Vlad Modoranu\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\losses.py:149 __call__\n        losses = ag_call(y_true, y_pred)\n    C:\\Users\\Ionut-Vlad Modoranu\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\losses.py:253 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    C:\\Users\\Ionut-Vlad Modoranu\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\Ionut-Vlad Modoranu\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\losses.py:1605 binary_crossentropy\n        K.binary_crossentropy(y_true, y_pred, from_logits=from_logits), axis=-1)\n    C:\\Users\\Ionut-Vlad Modoranu\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\Ionut-Vlad Modoranu\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\backend.py:4823 binary_crossentropy\n        return nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n    C:\\Users\\Ionut-Vlad Modoranu\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\Ionut-Vlad Modoranu\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:173 sigmoid_cross_entropy_with_logits\n        raise ValueError(\"logits and labels must have the same shape (%s vs %s)\" %\n\n    ValueError: logits and labels must have the same shape ((None, 1) vs (None, 2))\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-aeeea6fbb6f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'#################### BACKDOOR 0 ####################'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m \u001b[0mmodel_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_nn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_0_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_0_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_0_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_0_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'#################### BACKDOOR 1 ####################'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[0mmodel_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_nn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_1_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_1_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_1_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_1_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-34-aeeea6fbb6f8>\u001b[0m in \u001b[0;36mtrain_nn\u001b[1;34m(data_x, data_y, test_x, test_y)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;31m# dict(enumerate(class_weight.compute_class_weight('balanced', np.unique(data_y), data_y)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_y_onehot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Class weights:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    821\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 696\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    697\u001b[0m             *args, **kwds))\n\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\Ionut-Vlad Modoranu\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\Ionut-Vlad Modoranu\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\Ionut-Vlad Modoranu\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\Ionut-Vlad Modoranu\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\Ionut-Vlad Modoranu\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\Ionut-Vlad Modoranu\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\Ionut-Vlad Modoranu\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:748 train_step\n        loss = self.compiled_loss(\n    C:\\Users\\Ionut-Vlad Modoranu\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:204 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    C:\\Users\\Ionut-Vlad Modoranu\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\losses.py:149 __call__\n        losses = ag_call(y_true, y_pred)\n    C:\\Users\\Ionut-Vlad Modoranu\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\losses.py:253 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    C:\\Users\\Ionut-Vlad Modoranu\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\Ionut-Vlad Modoranu\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\losses.py:1605 binary_crossentropy\n        K.binary_crossentropy(y_true, y_pred, from_logits=from_logits), axis=-1)\n    C:\\Users\\Ionut-Vlad Modoranu\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\Ionut-Vlad Modoranu\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\backend.py:4823 binary_crossentropy\n        return nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n    C:\\Users\\Ionut-Vlad Modoranu\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\Ionut-Vlad Modoranu\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:173 sigmoid_cross_entropy_with_logits\n        raise ValueError(\"logits and labels must have the same shape (%s vs %s)\" %\n\n    ValueError: logits and labels must have the same shape ((None, 1) vs (None, 2))\n"
     ]
    }
   ],
   "source": [
    "def create_neural_network(input_size):\n",
    "    model = Sequential()\n",
    "#     model.add(Dense(10, input_dim=input_size, activation='softmax')) # logistic regression: only one dense layer with 7 units, input_size and softmax    \n",
    "\n",
    "    model.add(Dense(10, input_dim=input_size, activation='relu')) # logistic regression: only one dense layer with 7 units, input_size and softmax\n",
    "#     model.add(Dense(30, activation='relu'))\n",
    "#     model.add(Dense(7, activation='softmax'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def cross_validation(data_x, data_y):\n",
    "    n_splits = 4\n",
    "    n_repeats = 3\n",
    "    pipeline = Pipeline([('scale', MinMaxScaler()), ('mlp', KerasClassifier(build_fn=create_neural_network, epochs=100, batch_size=32, verbose=0))])\n",
    "    for _ in range(3):\n",
    "        pipeline = Pipeline([('mlp', KerasClassifier(build_fn=create_neural_network, epochs=50, batch_size=16, verbose=0))])\n",
    "        outer_kfold = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=666)\n",
    "        for idx_train_val, idx_test in outer_kfold.split(data_x, data_y):\n",
    "            subset_data_x = data_x[idx_train_val, :]\n",
    "            subset_data_y = data_y[idx_train_val]\n",
    "            inner_kfold = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=666)\n",
    "# #     weights = np.ones((7,))\n",
    "# #     weights = class_weight.compute_class_weight('balanced', np.unique(data_y), data_y)#/ n_splits\n",
    "#     pipeline = Pipeline([('mlp', KerasClassifier(build_fn=create_neural_network, epochs=50, batch_size=8, verbose=1))])\n",
    "#     kfold = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=666)\n",
    "#     results = cross_val_score(pipeline, data_x, data_y, cv=kfold, scoring='accuracy', n_jobs=-1)#, fit_params={'sample_weight': weights}) \n",
    "#     print(f'CV: acc mean={results.mean():.3f}, acc std={results.std():.3f}')\n",
    "# #     results = results[~np.isnan(results)]\n",
    "# #     results = results[~np.isinf(results)]\n",
    "# #     print(f'CV: CE mean={-results.mean():.3f} CE std={results.std():.3f}')\n",
    "    \n",
    "def train_nn(data_x, data_y, test_x=None, test_y=None):\n",
    "    data_y_onehot = OneHotEncoder().fit_transform(data_y.reshape(-1,1)).toarray()\n",
    "    if test_y is not None:\n",
    "        test_y_onehot = OneHotEncoder().fit_transform(test_y.reshape(-1,1)).toarray()\n",
    "    \n",
    "#     scaler = StandardScaler()\n",
    "#     scaler.fit(data_x[:,:12])\n",
    "#     data_x = scaler.transform(data_x[:,:12])\n",
    "#     test_x = scaler.transform(test_x[:,:12])\n",
    "\n",
    "    # convert labels from {0, ..., 6} to {0, 1}. Backdoored classes in {1, ..., 6} are mapped to class 1 (simply backdoored)\n",
    "    ## for training data\n",
    "    data_y_binarized = np.copy(data_y)\n",
    "    data_y_binarized[data_y_binarized > 0] = 1\n",
    "    ## for holdout data\n",
    "    if test_y is not None:\n",
    "        test_y_binarized = np.copy(test_y)\n",
    "        test_y_binarized[test_y_binarized > 0] = 1\n",
    "    \n",
    "    model = create_neural_network(input_size=data_x.shape[1])\n",
    "    \n",
    "    weights = None # dict(enumerate(class_weight.compute_class_weight('balanced', np.unique(data_y), data_y)))\n",
    "    history = model.fit(data_x, data_y_onehot, epochs=100, batch_size=8, verbose=0, class_weight=weights)\n",
    "    print('Class weights:', weights)\n",
    "    \n",
    "    # get one-hot prediction on training data\n",
    "    pred_train = model.predict(data_x)\n",
    "    \n",
    "    # get predicted classes (index of maximum probability)\n",
    "    pred_train_labels = np.argmax(pred_train, axis=1)\n",
    "    \n",
    "    # convert labels from {0, ..., 6} to {0, 1}. Backdoored classes in {1, ..., 6} are mapped to class 1 (simply backdoored)\n",
    "    pred_train_binarized = np.copy(pred_train_labels)\n",
    "    pred_train_binarized[pred_train_binarized > 0] = 1\n",
    "\n",
    "    # keep clean probability as it is and compute the backdoored probability as 1-clean probability just like we had binary classification\n",
    "    pred_train_probas = np.zeros((data_x.shape[0], 2))\n",
    "    pred_train_probas[:, 0] = pred_train[:, 0]\n",
    "    pred_train_probas[:, 1] = 1 - pred_train[:, 0]\n",
    "    \n",
    "    loss, accuracy = model.evaluate(data_x, data_y_onehot, verbose=0)\n",
    "    train_ce = log_loss(y_true=data_y_binarized, y_pred=pred_train_probas)\n",
    "    print(f'\\nTrain Loss:      {loss:.3f}')\n",
    "    print(f'Train Accuracy:  {accuracy_score(y_true=data_y, y_pred=pred_train_labels):.3f}')\n",
    "    print(f'Train Precision: {precision_score(y_true=data_y_binarized, y_pred=pred_train_binarized):.3f}')\n",
    "    print(f'Train Recall:    {recall_score(y_true=data_y_binarized, y_pred=pred_train_binarized):.3f}')\n",
    "    print(f'Train Log-Loss:  {train_ce:.3f}')\n",
    "    print(f'Train Confusion Matrix (all):\\n{confusion_matrix(y_true=data_y, y_pred=pred_train_labels)}')\n",
    "    print(f'Train Confusion Matrix (binary):\\n{confusion_matrix(y_true=data_y_binarized, y_pred=pred_train_binarized)}')\n",
    "    \n",
    "    if test_x is not None and test_y is not None:\n",
    "        pred_holdout = model.predict(test_x)\n",
    "        \n",
    "        # get one-hot prediction on training data\n",
    "        pred_holdout_labels = np.argmax(pred_holdout, axis=1)\n",
    "        \n",
    "        # convert labels from {0, ..., 6} to {0, 1}. Backdoored classes in {1, ..., 6} are mapped to class 1 (simply backdoored)\n",
    "        pred_holdout_binarized = np.copy(pred_holdout_labels)\n",
    "        pred_holdout_binarized[pred_holdout_binarized > 0] = 1\n",
    "        \n",
    "        # keep clean probability as it is and compute the backdoored probability as 1-clean probability just like we had binary classification\n",
    "        pred_holdout_probas = np.zeros((test_x.shape[0], 2))\n",
    "        pred_holdout_probas[:, 0] = pred_holdout[:, 0]\n",
    "        pred_holdout_probas[:, 1] = 1 - pred_holdout[:, 0]\n",
    "        \n",
    "        loss, accuracy = model.evaluate(test_x, test_y_onehot, verbose=0)\n",
    "        holdout_ce = log_loss(y_true=test_y_binarized, y_pred=pred_holdout_probas)\n",
    "        print(f'\\nTest Loss:      {loss:.3f}')\n",
    "        print(f'Test Accuracy:  {accuracy_score(y_true=test_y, y_pred=pred_holdout_labels):.3f}')\n",
    "        print(f'Test Precision: {precision_score(y_true=test_y_binarized, y_pred=pred_holdout_binarized):.3f}')\n",
    "        print(f'Test Recall:    {recall_score(y_true=test_y_binarized, y_pred=pred_holdout_binarized):.3f}')\n",
    "        print(f'Test Log-Loss:  {holdout_ce:.3f} <========')\n",
    "        print(f'Test Confusion Matrix (all):\\n{confusion_matrix(y_true=test_y, y_pred=pred_holdout_labels)}')\n",
    "        print(f'Test Confusion Matrix (binary):\\n{confusion_matrix(y_true=test_y_binarized, y_pred=pred_holdout_binarized)}')\n",
    "        \n",
    "#     folder = r'D:\\Cloud\\MEGA\\TrojAI\\TrojAI-UMD\\metamodels\\metamodel_10_round3_NN-60-30_min-max-scaled_RAW_square30_RANDOM_filters_all-classes'\n",
    "#     folder = r'D:\\Cloud\\MEGA\\TrojAI\\TrojAI-UMD\\metamodels\\metamodel_11_round3_NN-60-30_min-max-scaled_RAW_square30_GRAY'\n",
    "\n",
    "#     the_time = datetime.now().strftime('%Y-%m-%d_%Hh%Mm%Ss')\n",
    "#     folder = fr'neural_networks_separate_backdoors_round_4/{the_time}_train-CE={train_ce:.3f}_holdout-CE={holdout_ce:.3f}_input={data_x.shape[1]}'\n",
    "#     keras_save(model, folder)\n",
    "    \n",
    "#     save_obj(scaler, os.path.join(folder, 'scaler.pickle'))\n",
    "    return model\n",
    "\n",
    "path_train = r'confusion-reports\\ics_fc\\round4-train-dataset\\round4-train-dataset_fc_synthetic_polygon-all-gray_filters_0-1007.csv'\n",
    "\n",
    "use_arch = True\n",
    "X_0, y_0 = read_features(path_train, trigger_type_aux_str=None, arch=None, data='diffs', label_type='binary', append_arch=use_arch, arch_one_hot=use_arch)\n",
    "X_1, y_1 = read_features(path_train, trigger_type_aux_str=None, arch=None, data='diffs', label_type='binary', append_arch=use_arch, arch_one_hot=use_arch)\n",
    "\n",
    "percent_training = 0.7\n",
    "n_samples = X_0.shape[0]\n",
    "np.random.seed(int(time.time()))\n",
    "# np.random.seed(666)\n",
    "indexes_train = np.random.choice(n_samples, int(n_samples * percent_training), replace=False).tolist()\n",
    "indexes_test = [i for i in range(n_samples) if i not in indexes_train]\n",
    "print('indexes_test:', indexes_test)\n",
    "\n",
    "X_0_train, y_0_train, X_0_test, y_0_test = X_0[indexes_train, :], y_0[indexes_train], X_0[indexes_test, :], y_0[indexes_test]\n",
    "X_1_train, y_1_train, X_1_test, y_1_test = X_1[indexes_train, :], y_1[indexes_train], X_1[indexes_test, :], y_1[indexes_test]\n",
    "\n",
    "print(X_0_train.shape, y_0_train.shape, X_0_test.shape, y_0_test.shape)\n",
    "# print(X_0_train[0, :], y_0_train[0])\n",
    "# print(X_0_test[0, :], y_0_test[0])\n",
    "# print()\n",
    "print(X_1_train.shape, y_1_train.shape, X_1_test.shape, y_1_test.shape)\n",
    "# print(X_1_train[0, :], y_1_train[0])\n",
    "# print(X_1_test[0, :], y_1_test[0])\n",
    "\n",
    "print('#################### BACKDOOR 0 ####################')\n",
    "model_0 = train_nn(X_0_train, y_0_train, X_0_test, y_0_test)\n",
    "print('#################### BACKDOOR 1 ####################')\n",
    "model_1 = train_nn(X_1_train, y_1_train, X_1_test, y_1_test)\n",
    "print(f'Using arch: {use_arch}')\n",
    "\n",
    "pred_0 = model_0.predict(X_0_test)\n",
    "pred_1 = model_1.predict(X_1_test)\n",
    "\n",
    "# pred_0 = model_0.predict(X_0_train)\n",
    "# pred_1 = model_1.predict(X_1_train)\n",
    "\n",
    "df = pd.DataFrame(columns=['true_binary_label', 'true_backd_0', 'true_backd_1', 'pred_backd_0', 'pred_backd_1', 'backdoor_proba', 'proba_0', 'proba_1'])\n",
    "ce_loss = 0.0\n",
    "for i in range(pred_0.shape[0]):\n",
    "    t0, t1 = y_0_test[i], y_1_test[i]\n",
    "#     t0, t1 = y_0_train[i], y_1_train[i]\n",
    "    p0, p1 = pred_0[i, :], pred_1[i, :]\n",
    "    label0, label1 = np.argmax(p0), np.argmax(p1)\n",
    "    if label0 + label1 == 0: # both models predicted 'clean'\n",
    "        backdoor_proba = 1.0 - (p0[0] + p1[0]) / 2.0 # mean of the two clean predictions (1 - value) = backdoor probability\n",
    "    elif label0 == 0:\n",
    "        backdoor_proba = p1[1:].sum()\n",
    "    elif label1 == 0:\n",
    "        backdoor_proba = p0[1:].sum()\n",
    "    else:\n",
    "        backdoor_proba = (p0[1:].sum() + p1[1:].sum()) / 2.0\n",
    "    t = 0 if t0 + t1 == 0 else 1\n",
    "    ce_loss += -(t * np.log(backdoor_proba) + (1-t) * np.log(1.0 - backdoor_proba))\n",
    "    df.loc[i] = [t, t0, t1, label0, label1, backdoor_proba, list(map(lambda x: round(x,2), p0.tolist())), list(map(lambda x: round(x,2), p1.tolist()))]\n",
    "ce_loss = ce_loss / pred_0.shape[0]\n",
    "print(f'CE loss = {ce_loss}')\n",
    "the_time = datetime.now().strftime('%Y-%m-%d_%Hh%Mm%Ss')\n",
    "folder = fr'neural_networks_separate_backdoors_round_4/{the_time}_test-CE={ce_loss:.3f}_input={X_0.shape[1]}_output={X_0.shape}_train-count={len(indexes_train)}_test-count={len(indexes_test)}'\n",
    "keras_save(model_0, folder, name='model_0')\n",
    "keras_save(model_1, folder, name='model_1')\n",
    "print('done')\n",
    "# df[['true_binary_label', 'true_backd_0', 'true_backd_1', 'pred_backd_0', 'pred_backd_1']] = df[['true_binary_label', 'true_backd_0', 'true_backd_1', 'pred_backd_0', 'pred_backd_1']].astype('int')\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_csv = fr'confusion-reports\\ics_svm\\round3-train-dataset\\round3-train-dataset_square-30-random_filters.csv'\n",
    "# path_csv = fr'confusion-reports\\ics_fc\\round3-train-dataset\\round3-train-dataset_fc_square-30-random_filters.csv'\n",
    "path_csv = fr'confusion-reports/conf_mat/round3-train-dataset_fc_square-30-gray_filters_h_kl.csv'\n",
    "df = pd.read_csv(path_csv)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_csv = fr'confusion-reports/conf_mat/round3-train-dataset_fc_square-30-random_filters_h_kl.csv'\n",
    "df = pd.read_csv(path_csv)\n",
    "labels = df['model_label']\n",
    "for x in ['model_name', 'model_architecture', 'architecture_code', 'trigger_color', 'num_classes', 'trigger_type_aux']:\n",
    "    del df[x]\n",
    "for k in ['clean', 'square30', 'gotham', 'kelvin', 'lomo', 'nashville', 'toaster']:\n",
    "    df[f'ch_{k}'] = df[f'h_{k}'] + df[f'kl_{k}']\n",
    "#     del df[f'h_{k}'], df[f'kl_{k}']\n",
    "plt.figure(figsize=(40,20)).patch.set_color('white')\n",
    "sns.heatmap(df.corr(), annot=True, cmap='YlGnBu')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_csv = fr'confusion-reports/conf_mat/round3-train-dataset_fc_square-30-gray_filters_h_kl.csv'\n",
    "path_csv = fr'confusion-reports/conf_mat/round3-train-dataset_fc_square-30-random_filters_h_kl.csv'\n",
    "# path_csv = fr'confusion-reports\\ics_fc\\round3-train-dataset\\round3-train-dataset_fc_square-30-random_filters.csv'\n",
    "X, y = read_features(path_csv, trigger_type_aux_str=None, label_type='binary', arch=None, data='hkl', append_arch=False, arch_one_hot=False)\n",
    "# X, y = read_features(path_csv, trigger_type_aux_str='polygon', arch=None, data='diffs', append_arch=False, arch_one_hot=False)\n",
    "print(X.shape)\n",
    "# X = StandardScaler().fit_transform(X)\n",
    "arr = ['h_clean', 'kl_clean', 'h_square30', 'kl_square30', 'h_gotham', 'kl_gotham', 'h_kelvin', 'kl_kelvin', 'h_lomo', 'kl_lomo', 'h_nashville', 'kl_nashville', 'h_toaster', 'kl_toaster']\n",
    "# arr = ['square30_r_mean_diff', 'square30_r_std_diff', 'gotham_mean_diff', 'gotham_std_diff', 'kelvin_mean_diff', 'kelvin_std_diff', 'lomo_mean_diff', 'lomo_std_diff', 'nashville_mean_diff', 'nashville_std_diff', 'toaster_mean_diff', 'toaster_std_diff']\n",
    "print(list(zip(arr, X[0,:].tolist())))\n",
    "\n",
    "i, j = arr.index('kl_square30'), arr.index('kl_toaster')\n",
    "# i, j = arr.index('square30_r_mean_diff'), arr.index('square30_r_std_diff')\n",
    "\n",
    "f1, f2 = abs(X[:, i]), abs(X[:, j])\n",
    "f1 = np.log(f1)\n",
    "f2 = np.log(f2)\n",
    "plt.figure(figsize=(20,15)).patch.set_color('white')\n",
    "plt.scatter(f1, f2, c=[['blue', 'red'][vy] for vy in y], s=15.0)\n",
    "# plt.legend()\n",
    "plt.grid()\n",
    "# Good results for H+KL:\n",
    "# indexes=3,4, log(|f1|), log(|f2|), h+kl (kl_square30, h_gotham)\n",
    "# indexes=3,5 (kl_square30, kl_gotham)\n",
    "# indexes=3,5 (kl_square30, kl_toaster)\n",
    "\n",
    "# with standardization:\n",
    "# 3, 4, log f1, log f2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# path_csv = fr'confusion-reports\\ics_svm\\round3-train-dataset\\round3-train-dataset_square-30-filters_all-classes_gray.csv'\n",
    "# path_csv = fr'confusion-reports\\ics_fc\\round3-train-dataset\\round3-train-dataset_fc_square-30-gray_filters.csv'\n",
    "# path_csv = fr'confusion-reports\\ics_fc\\round3-train-dataset\\round3-train-dataset_fc_square-30-random_filters.csv'\n",
    "# path_csv = fr'confusion-reports/conf_mat/round3-train-dataset_fc_square-30-gray_filters_h_kl.csv'\n",
    "path_csv = fr'confusion-reports/conf_mat/round3-train-dataset_fc_square-30-random_filters_h_kl.csv'\n",
    "df = pd.read_csv(path_csv)\n",
    "df = df[df['model_architecture'].str.contains('vgg')]\n",
    "df['model_label'] = 1 - df['model_label'].astype('category').cat.codes\n",
    "del df['num_classes']\n",
    "indexes = []\n",
    "arch = None\n",
    "if arch is not None:\n",
    "    for i in range(len(df)):\n",
    "        if df['model_architecture'].iloc[i].startswith(arch):\n",
    "            indexes.append(i)\n",
    "    df = df.iloc[indexes]\n",
    "for c in df.columns:\n",
    "    if c.endswith('mean') or c.endswith('std'):# or c.startswith('h'):\n",
    "#     if c.endswith('diff'):\n",
    "        del df[c]\n",
    "print(df.shape)\n",
    "print(df.iloc[0])\n",
    "\n",
    "for k in ['clean', 'square30', 'gotham', 'kelvin', 'lomo', 'nashville', 'toaster']:\n",
    "    df[f'kl_{k}'] = np.log(df[f'kl_{k}'])\n",
    "    df[f'h_{k}'] = np.log(df[f'h_{k}'])\n",
    "\n",
    "plt.figure(figsize=(40,20)).patch.set_color('white')\n",
    "sns.heatmap(df.corr(), annot=True, cmap='YlGnBu')\n",
    "\n",
    "# plt.figure(figsize=(30,12)).patch.set_color('white')\n",
    "# sns.pairplot(df, hue='model_label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box-plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# path_csv = fr'confusion-reports\\ics_svm\\round3-train-dataset\\round3-train-dataset_square-30-filters_all-classes_gray.csv'\n",
    "# path_csv = fr'confusion-reports\\ics_fc\\round3-train-dataset\\round3-train-dataset_fc_square-30-gray_filters.csv'\n",
    "path_csv = fr'confusion-reports\\ics_fc\\round3-train-dataset\\round3-train-dataset_fc_square-30-random_filters.csv'\n",
    "# path_csv = fr'confusion-reports/conf_mat/round3-train-dataset_fc_square30-gray_filters_h_kl.csv'\n",
    "df = pd.read_csv(path_csv)\n",
    "df['model_label'] = 1 - df['model_label'].astype('category').cat.codes\n",
    "del df['num_classes'], df['model_name']\n",
    "indexes = []\n",
    "arch = None\n",
    "if arch is not None:\n",
    "    for i in range(len(df)):\n",
    "        if df['model_architecture'].iloc[i].startswith(arch):\n",
    "            indexes.append(i)\n",
    "    df = df.iloc[indexes]\n",
    "for c in df.columns:\n",
    "    if c.endswith('mean') or c.endswith('std'):\n",
    "#     if c.endswith('diff'):\n",
    "        del df[c]\n",
    "del df['model_architecture'], df['architecture_code'], df['trigger_type_aux'], df['trigger_color']#, df['model_label']\n",
    "print(df.columns.tolist())\n",
    "for c in df.columns:\n",
    "#     if c.startswith('h'):\n",
    "    if c.endswith('mean_diff'):\n",
    "        plt.figure().patch.set_color('white')\n",
    "        sns.boxplot(x=\"model_label\", y=c, hue=\"model_label\", data=df, palette=\"Set3\")\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_csv = fr'confusion-reports\\ics_svm\\round3-train-dataset\\round3-train-dataset_square-30-filters_all-classes_gray.csv'\n",
    "path_csv = fr'confusion-reports\\ics_fc\\round3-train-dataset\\round3-train-dataset_fc_square-30-gray_filters.csv'\n",
    "df = pd.read_csv(path_csv)\n",
    "df['model_label'] = 1 - df['model_label'].astype('category').cat.codes\n",
    "indexes = []\n",
    "arch = None\n",
    "if arch is not None:\n",
    "    for i in range(len(df)):\n",
    "        if df['model_architecture'].iloc[i].startswith(arch):\n",
    "            indexes.append(i)\n",
    "    df = df.iloc[indexes]\n",
    "for c in df.columns:\n",
    "    if c.endswith('mean') or c.endswith('std'):\n",
    "#     if c.endswith('diff'):\n",
    "        del df[c]\n",
    "del df['model_architecture'], df['trigger_type_aux'], df['trigger_color'], df['num_classes'], df['model_name']\n",
    "# print(df.head())\n",
    "\n",
    "data = df.values\n",
    "X, y = data[:, 1:], data[:, 0]\n",
    "iso = IsolationForest(contamination=0.1)\n",
    "yhat = iso.fit_predict(X)\n",
    "mask = np.where(yhat == 1)[0]\n",
    "print(mask.shape)\n",
    "del df['model_label']\n",
    "plt.figure(figsize=(30, 12)).patch.set_color('white')\n",
    "df.iloc[mask].boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratified K-fold validation for a full training dataset (square-size and 5 filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_splits = 10\n",
    "n_repeats = 5\n",
    "\n",
    "for size in [30]: #[10, 15, 20, 25, 30, 35, 40, 45, 50]:\n",
    "#     path_csv = fr'confusion-reports\\ics_svm\\round3-train-dataset\\round3-train-dataset_square-{size}_filters.csv'\n",
    "#     path_csv = fr'confusion-reports\\ics_svm\\round3-train-dataset\\round3-train-dataset_square-{size}-random_filters.csv'\n",
    "#     path_csv = fr'confusion-reports\\ics_fc\\round3-train-dataset\\round3-train-dataset_fc_square-{size}-gray_filters.csv'\n",
    "#     path_csv = fr'confusion-reports\\ics_fc\\round3-train-dataset\\round3-train-dataset_fc_square-{size}-random_filters.csv'\n",
    "#     path_csv = fr'confusion-reports/conf_mat/round3-train-dataset_fc_square-{size}-gray_filters_h_kl.csv'\n",
    "    path_csv = fr'confusion-reports/conf_mat/round3-train-dataset_fc_square-{size}-random_filters_h_kl.csv'\n",
    "    for A in [None]: #['vgg', 'wideresnet', 'resnet', 'densenet', 'mobilenet', 'shufflenet', 'squeezenet', 'inception', 'googlenet']:\n",
    "        X, y = read_features(path_csv, trigger_type_aux_str=None, arch=None, data='hkl', append_arch=False, arch_one_hot=False) #['vgg', 'resnet' (includes wideresnet), 'densenet', 'mobilenet', 'shufflenet', 'squeezenet', 'inception', 'googlenet', None]:\n",
    "    #     X, y = X[2:,[0, 2, 4, 6]], y[2:]\n",
    "    #     X, y = X[2:,:], y[2:]\n",
    "        scores_roc, scores_xent = [], []\n",
    "        print(X.shape, y.shape)\n",
    "        print(X[0,:].tolist())\n",
    "        try:\n",
    "            for _ in range(n_repeats):\n",
    "                kfold =  StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "                for train_index, test_index in kfold.split(X, y):\n",
    "                    X_train, y_train = X[train_index, :], y[train_index]\n",
    "                    X_test, y_test = X[test_index, :], y[test_index]\n",
    "\n",
    "    #                 model = RandomForestClassifier(n_estimators=100)\n",
    "                    model = RandomForestClassifier(n_estimators=500)\n",
    "    #                 model = GradientBoostingClassifier(n_estimators=100, loss='deviance')\n",
    "    #                 model = GradientBoostingClassifier(n_estimators=100, loss='exponential')\n",
    "    #                 model = svm.SVC(C=1, probability=True, kernel='rbf')\n",
    "\n",
    "#                     model = LogisticRegression(C=0.5)\n",
    "#                     scaler = StandardScaler()\n",
    "#                     scaler.fit(X_train)\n",
    "#                     X_train = scaler.transform(np.copy(X_train))\n",
    "#                     X_test = scaler.transform(np.copy(X_test))\n",
    "\n",
    "                    roc, xent = evaluate_classifier(model, X_train, y_train, X_test, y_test)\n",
    "                    scores_roc.append(roc)\n",
    "                    scores_xent.append(xent)\n",
    "        except ValueError as e:\n",
    "            print('ERROR')\n",
    "            print(e)\n",
    "        print(f'size={size}')\n",
    "        print(f'CE mean={np.mean(scores_xent):.3f}, CE std={np.std(scores_xent):.3f}')\n",
    "        print('-------------------------------------------------------------------------')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPO for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_csv = fr'confusion-reports\\ics_fc\\round3-train-dataset\\round3-train-dataset_fc_square-30-gray_filters.csv'\n",
    "path_csv = fr'confusion-reports\\ics_fc\\round3-train-dataset\\round3-train-dataset_fc_square-30-random_filters.csv'\n",
    "# path_csv = fr'confusion-reports\\ics_svm\\round3-train-dataset\\round3-train-dataset_square-30_filters.csv'\n",
    "# path_csv = fr'confusion-reports\\ics_svm\\round3-train-dataset\\round3-train-dataset_square-30-random_filters.csv'\n",
    "X, y = read_features(path_csv, trigger_type_aux_str=None, arch=None, data='diffs', append_arch=True)\n",
    "print(X.shape)\n",
    "params = {\n",
    "    \"C\": [0.01 + i * 0.01 for i in range(1000)]\n",
    "}\n",
    "grid_cv = GridSearchCV(estimator=LogisticRegression(),\n",
    "                       param_grid=params,\n",
    "                       cv=RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1),\n",
    "                       scoring=make_scorer(log_loss, greater_is_better=False, needs_proba=True),\n",
    "                       n_jobs=-1)\n",
    "grid_cv.fit(X, y)\n",
    "\n",
    "# print('Best params:', grid_cv.best_params_)\n",
    "# print('Best CE:', grid_cv.best_score_)\n",
    "pd.DataFrame.from_dict({\n",
    "    'C': [d['C'] for d in grid_cv.cv_results_['params']],\n",
    "    'CE': -grid_cv.cv_results_['mean_test_score']\n",
    "}).sort_values(['CE'], ascending=True, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train meta-model using square data and filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# path_csv = fr'confusion-reports\\ics_fc\\round3-train-dataset\\round3-train-dataset_fc_square-30-gray_filters.csv'\n",
    "path_csv = fr'confusion-reports\\ics_fc\\round3-train-dataset\\round3-train-dataset_fc_square-30-random_filters.csv'\n",
    "# path_csv = fr'confusion-reports\\ics_svm\\round3-train-dataset\\round3-train-dataset_square-30-random_filters.csv'\n",
    "# path_csv = fr'confusion-reports/conf_mat/round3-train-dataset_fc_square-30-random_filters_h_kl.csv'\n",
    "for arch in ['vgg', 'resnet', 'densenet', 'mobilenet', 'shufflenet', 'squeezenet', 'inception', 'googlenet']:\n",
    "    X_train, y_train = read_features(path_csv, trigger_type_aux_str=None, arch=arch, data='diffs', append_arch=False, arch_one_hot=False)\n",
    "#     X_train = X_train[2:, :]\n",
    "#     y_train = y_train[2:]\n",
    "    print(arch, X_train.shape)\n",
    "#     scaler=None\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "\n",
    "#     model = RandomForestClassifier(n_estimators=500)\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    name = '../metamodels/metamodel_16_fc_round3_data=diffs_square=30-random_scaler=std_clf=LR-1_arch-features=no_arch-wise-models=yes'\n",
    "#     if os.path.isdir(name):\n",
    "#         shutil.rmtree(name)\n",
    "    if not os.path.isdir(name):\n",
    "        os.mkdir(name)\n",
    "    save_obj(model, f'{name}/model-{arch}.pickle')\n",
    "    save_obj(scaler, f'{name}/scaler-{arch}.pickle')\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_csv = fr'confusion-reports\\ics_fc\\round3-train-dataset\\round3-train-dataset_fc_square-30-gray_filters.csv'\n",
    "\n",
    "X, y = read_features(path_csv, trigger_type_aux_str=None, arch=None, data='diffs')\n",
    "X_train, y_train = X[:500, :], y[:500]\n",
    "X_test, y_test = X[500:, :], y[500:]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "model = LogisticRegression(C=1)\n",
    "model.fit(X_train, y_train)\n",
    "ce = 0.0\n",
    "for i in range(X_test.shape[0]):\n",
    "    prob = model.predict_proba(X_test[i,:].reshape(1, -1))\n",
    "    p0, p1 = prob[0]\n",
    "    t = y[i]\n",
    "    local_ce = -(t * np.log2(p1) + (1-t) * np.log2(1-p0))\n",
    "#     print(i, local_ce)\n",
    "    ce += local_ce\n",
    "print(ce / X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Testing: train on training data and test on holdout data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_csv = r'confusion-reports\\ics_svm\\round2-train-dataset\\round2-train-dataset_square-25-filters_all-classes_gray.csv'\n",
    "path_holdout_csv = r'confusion-reports\\ics_svm\\round2-holdout-dataset\\round2-holdout-dataset_square-25-filters_all-classes_gray.csv'\n",
    "# path_train_csv = r'confusion-reports\\ics_svm\\round2-train-dataset\\round2-train-dataset_square-25-filters_all-classes_gray_confusion-matrix.csv'\n",
    "# path_holdout_csv = r'confusion-reports\\ics_svm\\round2-holdout-dataset\\round2-holdout-dataset_square-25-filters_all-classes_gray_confusion-matrix.csv'\n",
    "trigger_type_aux_str = None\n",
    "\n",
    "print('Local Testing')\n",
    "if 'confusion-matrix' in path_train_csv and 'confusion-matrix' in path_holdout_csv:\n",
    "    print('Approach: confusion matrix and original CNN')\n",
    "    X_train, y_train = read_features_confusion_matrix(path_train_csv, trigger_type_aux_str)\n",
    "    X_holdout, y_holdout = read_features_confusion_matrix(path_holdout_csv, trigger_type_aux_str)\n",
    "else:\n",
    "    print('Approach: confusion distribution and SDNs')\n",
    "    X_train, y_train = read_features(path_train_csv, trigger_type_aux_str)\n",
    "    X_holdout, y_holdout = read_features(path_holdout_csv, trigger_type_aux_str)\n",
    "\n",
    "print('train shape:', X_train.shape, y_train.shape)\n",
    "print('holdout shape:', X_holdout.shape, y_holdout.shape)\n",
    "\n",
    "roc, xent = evaluate_classifier(X_train, y_train, X_holdout, y_holdout)\n",
    "print(f'ROC AUC = {roc}')\n",
    "print(f'Cross-Entropy = {xent}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
