{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from methods import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def select_columns_from_dataset():\n",
    "    path = r'confusion-reports\\ics_fc\\round3-holdout-dataset\\round3-holdout-dataset_fc_square30-random_filters.csv'\n",
    "    df = pd.read_csv(path)\n",
    "#     for col_to_delete in ['square30_r_mean_diff', 'square30_r_std_diff', 'square30_r_mean', 'square30_r_std']:\n",
    "#     for col_to_delete in ['square30_r_mean_diff', 'square30_r_std_diff', 'square30_r_mean', 'square30_r_std']:\n",
    "#         del df[col_to_delete]\n",
    "    df_cols = df.columns\n",
    "    for col_to_delete in df_cols:\n",
    "        if 'mean' in col_to_delete or 'std' in col_to_delete:\n",
    "            del df[col_to_delete]\n",
    "    df.to_csv(r'confusion-reports\\round3-holdout-columns.csv', index=False)\n",
    "#     df.to_csv(f'{path.replace(\".csv\", \"\")}_changed.csv', index=False)\n",
    "\n",
    "def merge_datasets():\n",
    "#     for size in [10, 20, 25, 30, 35, 40, 45, 50]:\n",
    "#     for size in [25, 30, 35, 40, 45, 50]:\n",
    "    for size in [30]:\n",
    "        path = fr'confusion-reports\\ics_fc\\round3-holdout-dataset\\round3-holdout-dataset_fc_square-{size}-gray_filters.csv'\n",
    "#         path = fr'confusion-reports\\ics_fc\\round3-train-dataset\\round3-train-dataset_fc_square-{size}-random_filters.csv'\n",
    "#         path = fr'confusion-reports\\ics_svm\\round3-train-dataset\\round3-train-dataset_square-{size}_filters.csv'\n",
    "#         path = fr'confusion-reports\\conf_mat\\round3-train-dataset_fc_square-{size}-random_filters_h_kl.csv'\n",
    "        df_to_modify_and_save = pd.read_csv(path)\n",
    "        df_to_copy_columns_from = pd.read_csv(fr'confusion-reports\\round3-holdout-columns.csv')\n",
    "        df_to_modify_and_save['architecture_code'] = df_to_copy_columns_from['architecture_code']\n",
    "#         for col in [f'square{size}_mean_diff', f'square{size}_std_diff', f'square{size}_mean', f'square{size}_std']:\n",
    "#             df_to_modify_and_save[col] = df_to_copy_columns_from[col]\n",
    "        new_order_for_columns = [\n",
    "            'model_name', 'model_architecture', 'architecture_code', 'model_label', 'trigger_type_aux',\n",
    "\n",
    "            f'square{size}_mean_diff', f'square{size}_std_diff',\n",
    "#             f'square{size}_r_mean_diff', f'square{size}_r_std_diff',\n",
    "            'gotham_mean_diff', 'gotham_std_diff',\n",
    "            'kelvin_mean_diff', 'kelvin_std_diff',\n",
    "            'lomo_mean_diff', 'lomo_std_diff',\n",
    "            'nashville_mean_diff', 'nashville_std_diff',\n",
    "            'toaster_mean_diff', 'toaster_std_diff',\n",
    "\n",
    "            'clean_mean', 'clean_std',\n",
    "            f'square{size}_mean', f'square{size}_std',\n",
    "#             f'square{size}_r_mean', f'square{size}_r_std',\n",
    "            'gotham_mean', 'gotham_std',\n",
    "            'kelvin_mean', 'kelvin_std',\n",
    "            'lomo_mean', 'lomo_std',\n",
    "            'nashville_mean', 'nashville_std',\n",
    "            'toaster_mean', 'toaster_std',\n",
    "            \n",
    "#             'h_clean', 'kl_clean',\n",
    "#             'h_square30', 'kl_square30',\n",
    "#             'h_gotham', 'kl_gotham',\n",
    "#             'h_kelvin', 'kl_kelvin',\n",
    "#             'h_lomo', 'kl_lomo',\n",
    "#             'h_nashville', 'kl_nashville',\n",
    "#             'h_toaster', 'kl_toaster',\n",
    "\n",
    "#             'trigger_color', \n",
    "            'num_classes',\n",
    "        ]\n",
    "        df_to_modify_and_save = df_to_modify_and_save[new_order_for_columns]\n",
    "        df_to_modify_and_save.to_csv(path, index=False)\n",
    "\n",
    "def correct_column_in_dataset():\n",
    "    df_with_correct_column = pd.read_csv(r'confusion-reports\\ics_svm\\round3-train-dataset\\round3-columns.csv')\n",
    "    column = 'trigger_type_aux'\n",
    "    for size in [10, 15, 20, 25,30, 35, 40, 45, 50]:\n",
    "        file = fr'confusion-reports\\ics_svm\\round3-train-dataset\\round3-train-dataset_square-{size}-filters_all-classes_gray.csv'\n",
    "        df_to_be_corrected = pd.read_csv(file)\n",
    "        df_to_be_corrected[column] = df_with_correct_column[column]\n",
    "        df_to_be_corrected.to_csv(file, index=False)\n",
    "\n",
    "def create_architecture_code():\n",
    "    df_columns = pd.read_csv(r'confusion-reports\\round3-columns.csv')\n",
    "    list_codes = []\n",
    "    for i in range(len(df_columns)):\n",
    "        model_architecture = df_columns['model_architecture'].iloc[i]\n",
    "        arch_code = encode_architecture(model_architecture)\n",
    "        if arch_code is None:\n",
    "            print(f'Error at index {i}')\n",
    "            break\n",
    "        list_codes.append(arch_code)\n",
    "    df_columns['architecture_code'] = list_codes\n",
    "    df_columns = df_columns[['model_name', 'poisoned', 'model_architecture', 'architecture_code', 'trigger_type_aux', 'trigger_type', 'instagram_filter_type', 'polygon_side_count']]\n",
    "    df_columns.to_csv(r'confusion-reports\\round3-columns.csv', index=False)\n",
    "\n",
    "def create_columns_csv_from_metadata():\n",
    "    path_metadata = r'confusion-reports\\METADATA-holdout.csv'\n",
    "    df = pd.DataFrame(columns=['model_name','poisoned','model_architecture','architecture_code','trigger_type_aux','trigger_type','instagram_filter_type','polygon_side_count'])\n",
    "    n_df = 0\n",
    "    metadata = pd.read_csv(path_metadata)\n",
    "    for i in range(len(metadata)):\n",
    "        model_name = metadata['model_name'].iloc[i]\n",
    "        poisoned = metadata['poisoned'].iloc[i]\n",
    "        model_architecture = metadata['model_architecture'].iloc[i]\n",
    "        architecture_code = encode_architecture(model_architecture)\n",
    "        trigger_type = metadata['trigger_type'].iloc[i]\n",
    "        polygon_side_count = metadata['polygon_side_count'].iloc[i]\n",
    "        instagram_filter_type = metadata['instagram_filter_type'].iloc[i]\n",
    "        trigger_type_aux = get_trigger_type_aux_value(trigger_type, polygon_side_count, instagram_filter_type)\n",
    "        df.loc[n_df] = [model_name, poisoned, model_architecture, architecture_code, trigger_type_aux, trigger_type, instagram_filter_type, polygon_side_count]\n",
    "        n_df += 1\n",
    "    df.to_csv(r'confusion-reports\\round3-holdout-columns.csv', index=False)\n",
    "\n",
    "def add_column_for_backdoor_type():\n",
    "#     path = fr'confusion-reports\\ics_fc\\round3-holdout-dataset\\round3-holdout-dataset_fc_square-30-gray_filters.csv'\n",
    "    path = fr'confusion-reports\\ics_fc\\round3-train-dataset\\round3-train-dataset_fc_filters.csv'\n",
    "    df = pd.read_csv(path)\n",
    "    column_backdoor_code = []\n",
    "    for i in range(len(df)):\n",
    "        trigger_type_aux = df['trigger_type_aux'].iloc[i]\n",
    "        code = encode_backdoor(trigger_type_aux)\n",
    "        column_backdoor_code.append(code)    \n",
    "    df.insert(3, 'backdoor_code', column_backdoor_code)\n",
    "    df.to_csv(path, index=False)\n",
    "\n",
    "def add_backdoor_code_2_column():\n",
    "    path = r'confusion-reports\\ics_fc\\round4-train-dataset\\round4-train-dataset_fc_synthetic_polygon-all-gray_filters_T=0.0.csv'\n",
    "    df = pd.read_csv(path)\n",
    "    col_backd_code_2 = []\n",
    "    for i in range(len(df)):\n",
    "        backd_0 = df['backdoor_code_0'].iloc[i]\n",
    "        backd_1 = df['backdoor_code_1'].iloc[i]\n",
    "        backd_2 = None\n",
    "        if backd_0 == 0 and backd_1 == 0:\n",
    "            backd_2 = '(0,)'\n",
    "        else:\n",
    "            if backd_0 == 0:\n",
    "                backd_2 = str((backd_1,))\n",
    "            elif backd_1 == 0:\n",
    "                backd_2 = str((backd_0,))\n",
    "            else:\n",
    "                backd_2 = str(tuple(sorted([backd_0, backd_1]))).replace(' ', '')\n",
    "        col_backd_code_2.append(backd_2)\n",
    "    df.insert(5, 'backdoor_code_2', col_backd_code_2)\n",
    "    df.to_csv(path, index=False)\n",
    "\n",
    "def normalize_confusion_distribution_by_num_classes():\n",
    "    path = r'confusion-reports/ics_fc/round4-train-dataset/round4-train-dataset_fc_synthetic_polygon-all-gray_filters_T=0.5_normalized_by_num_classes.csv'\n",
    "    df = pd.read_csv(path)\n",
    "    for c in df.columns:\n",
    "        if 'mean' in c:\n",
    "            df[c] /= df['num_classes']\n",
    "        if 'std' in c:\n",
    "            df[c] /= (df['num_classes'] ** 2)\n",
    "    df.to_csv(path, index=False)\n",
    "        \n",
    "# merge_datasets()\n",
    "# select_columns_from_dataset()\n",
    "# correct_column_in_dataset()\n",
    "# create_architecture_code()\n",
    "# create_columns_csv_from_metadata()\n",
    "# add_column_for_backdoor_type()\n",
    "add_backdoor_code_2_column()\n",
    "# normalize_confusion_distribution_by_num_classes()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
