{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.metrics import BinaryCrossentropy\n",
    "from methods import *\n",
    "import torch\n",
    "import pickle, os\n",
    "import skopt\n",
    "from skopt import BayesSearchCV, gp_minimize\n",
    "from skopt.space import Integer\n",
    "from skopt.space import Real\n",
    "from skopt.space import Categorical\n",
    "from skopt.utils import use_named_args\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def get_predicted_label(model, image, device):\n",
    "    output = model(image.to(device))\n",
    "    softmax = nn.functional.softmax(out[0].cpu(), dim=0)\n",
    "    pred_label = out.max(1)[1].item()\n",
    "    return label\n",
    "\n",
    "def save_obj(obj, filename):\n",
    "    with open(filename, 'wb') as handle:\n",
    "        pickle.dump(obj, handle)\n",
    "\n",
    "def load_obj(filename):\n",
    "    if not os.path.isfile(filename):\n",
    "        print('Pickle {} does not exist.'.format(filename))\n",
    "        return None\n",
    "    with open(filename, 'rb') as handle:\n",
    "        obj = pickle.load(handle)\n",
    "    return obj\n",
    "\n",
    "def read_features(p_path, trigger_type_aux_str=None):\n",
    "    \"\"\"trigger_type_aux_str: used to select only rows from models with polygon or filter backdoors. If none, all data is chosen\"\"\"\n",
    "    report = pd.read_csv(p_path)\n",
    "    if trigger_type_aux_str is None:\n",
    "        print('Using all rows (clean, polygon and instagram filters)')\n",
    "    else:\n",
    "        print(f'Using only clean rows and {trigger_type_aux_str}-backdoored rows from the dataset')\n",
    "        indexes = []\n",
    "        for i in range(len(report)):\n",
    "            col = report['trigger_type_aux'].iloc[i]\n",
    "            if (trigger_type_aux_str in col.lower()) or (col.lower() == 'none'):\n",
    "                indexes.append(i)\n",
    "        report = report.iloc[indexes]\n",
    "    initial_columns = report.columns\n",
    "    col_model_label = report['model_label'].copy(deep=True)\n",
    "    for c in initial_columns:\n",
    "        if not c.endswith('mean_diff') and not c.endswith('std_diff'):\n",
    "            del report[c]\n",
    "    features = report.values\n",
    "    labels = np.array([int(col_model_label.iloc[i] == 'backdoor') for i in range(len(report))])\n",
    "    return abs(features), labels\n",
    "\n",
    "def read_features_confusion_matrix(p_path, trigger_type_aux_str=None):\n",
    "    \"\"\"trigger_type_aux_str: used to select only rows from models with polygon or filter backdoors. If none, all data is chosen\"\"\"\n",
    "    report = pd.read_csv(p_path)\n",
    "    initial_columns = report.columns\n",
    "    col_model_label = report['model_label'].copy(deep=True)\n",
    "    for c in initial_columns:\n",
    "        if not c.startswith('h_') and not c.startswith('kl_'):\n",
    "            del report[c]\n",
    "    features = report.values\n",
    "    labels = np.array([int(col_model_label.iloc[i] == 'backdoor') for i in range(len(report))])\n",
    "    return abs(features), labels\n",
    "\n",
    "def evaluate_classifier(train_x, train_y, test_x, test_y):\n",
    "    #clf = svm.SVC(C=11, kernel='rbf', gamma='scale', probability=True)\n",
    "    clf = LogisticRegression(C=2.0)\n",
    "    clf.fit(train_x, train_y)\n",
    "    y_score = clf.predict(test_x)\n",
    "    y_pred = clf.predict_proba(test_x)\n",
    "    roc_auc = roc_auc_score(y_true=test_y, y_score=y_score)\n",
    "    cross_entropy = log_loss(y_true=test_y, y_pred=y_pred)\n",
    "    return roc_auc, cross_entropy\n",
    "\n",
    "def get_base_classifier():\n",
    "    # return svm.SVC(C=11, kernel='rbf', gamma='scale', probability=True)\n",
    "    return LogisticRegression()\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratified K-fold validation for a full training dataset (square-size and 5 filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using all rows (clean, polygon and instagram filters)\n",
      "CrossEntropy: mean=0.5407713898374368 std=0.06386917641348638\n"
     ]
    }
   ],
   "source": [
    "# path_csv = r'confusion-reports\\ics_svm\\round2-train-dataset\\round2-train-dataset_square25-filters_triggered_classes.csv' # old dataset\n",
    "# path_csv = r'confusion-reports\\ics_svm\\round2-train-dataset\\round2-train-dataset_square-25-filters_all-classes_gray.csv'\n",
    "# path_csv = r'confusion-reports\\ics_svm\\round2-train-dataset\\round2-train-dataset_square-25-filters_all-classes_gray_confusion-matrix.csv'\n",
    "n_splits = 10\n",
    "n_repeats = 1\n",
    "trigger_type_aux_str = None\n",
    "\n",
    "for size in [30]: # [25, 30, 35, 40, 45, 50]:\n",
    "    path_csv = fr'confusion-reports\\ics_svm\\round3-train-dataset\\round3-train-dataset_square-{size}-random-filters_all-classes_gray.csv'\n",
    "    X, y = read_features(path_csv, trigger_type_aux_str) # clean data is automatically added\n",
    "    \n",
    "    pipeline = Pipeline([('standardize', StandardScaler()), ('LR', get_base_classifier())])\n",
    "    kfold = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=666)\n",
    "    results = cross_val_score(pipeline, X, y, cv=kfold, scoring='neg_log_loss', n_jobs=-1)\n",
    "    print(f'CrossEntropy: mean={-results.mean()} std={results.std()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPO for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approach: confusion distribution and SDNs\n",
      "Using all rows (clean, polygon and instagram filters)\n",
      "(1008, 12) (1008,)\n",
      "Best Score: 0.539\n",
      "Best Parameters: [1.9807706419673599]\n"
     ]
    }
   ],
   "source": [
    "# path_csv = r'confusion-reports\\ics_svm\\round2-train-dataset\\round2-train-dataset_square-25-filters_all-classes_gray.csv'\n",
    "path_csv = r'confusion-reports\\ics_svm\\round3-train-dataset\\round3-train-dataset_square-30-random-filters_all-classes_gray.csv'\n",
    "\n",
    "trigger_type_aux_str = None\n",
    "if 'confusion-matrix' in path_csv:\n",
    "    print('Approach: confusion matrix and original CNN')\n",
    "    X, y = read_features_confusion_matrix(path_csv, trigger_type_aux_str)\n",
    "else:\n",
    "    print('Approach: confusion distribution and SDNs')\n",
    "    X, y = read_features(path_csv, trigger_type_aux_str) # clean data is automatically added\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "search_space = list()\n",
    "## LogisticRegression params\n",
    "search_space.append(Real(0.001, 100.0, 'log-uniform', name='C'))\n",
    "\n",
    "## SVM params\n",
    "# search_space.append(Real(0.00001, 100.0, 'log-uniform', name='C'))\n",
    "# # search_space.append(Integer(1, 5, name='degree'))\n",
    "# # search_space.append(Real(0.00001, 100.0, 'log-uniform', name='gamma'))\n",
    "# search_space.append(Categorical(['scale'], name='gamma'))\n",
    "# # search_space.append(Categorical(['rbf'], name='kernel'))\n",
    "# search_space.append(Categorical(['rbf'], name='kernel')) # linear, poly, rbf, sigmoid\n",
    "\n",
    "@use_named_args(search_space)\n",
    "def evaluate_model(**params):\n",
    "    model = LogisticRegression()\n",
    "    model.set_params(**params)\n",
    "#     params['probability'] = True\n",
    "#     model = svm.SVC()\n",
    "#     model.set_params(**params)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    result = cross_val_score(model, X, y, cv=cv, n_jobs=-1, scoring='neg_log_loss')\n",
    "    estimate = np.mean(result)\n",
    "    return abs(estimate)\n",
    "\n",
    "result = gp_minimize(evaluate_model, search_space)\n",
    "print('Best Score: %.3f' % (result.fun))\n",
    "print('Best Parameters: %s' % (result.x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Testing: train on training data and test on holdout data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_csv = r'confusion-reports\\ics_svm\\round2-train-dataset\\round2-train-dataset_square-25-filters_all-classes_gray.csv'\n",
    "path_holdout_csv = r'confusion-reports\\ics_svm\\round2-holdout-dataset\\round2-holdout-dataset_square-25-filters_all-classes_gray.csv'\n",
    "# path_train_csv = r'confusion-reports\\ics_svm\\round2-train-dataset\\round2-train-dataset_square-25-filters_all-classes_gray_confusion-matrix.csv'\n",
    "# path_holdout_csv = r'confusion-reports\\ics_svm\\round2-holdout-dataset\\round2-holdout-dataset_square-25-filters_all-classes_gray_confusion-matrix.csv'\n",
    "trigger_type_aux_str = None\n",
    "\n",
    "print('Local Testing')\n",
    "if 'confusion-matrix' in path_train_csv and 'confusion-matrix' in path_holdout_csv:\n",
    "    print('Approach: confusion matrix and original CNN')\n",
    "    X_train, y_train = read_features_confusion_matrix(path_train_csv, trigger_type_aux_str)\n",
    "    X_holdout, y_holdout = read_features_confusion_matrix(path_holdout_csv, trigger_type_aux_str)\n",
    "else:\n",
    "    print('Approach: confusion distribution and SDNs')\n",
    "    X_train, y_train = read_features(path_train_csv, trigger_type_aux_str)\n",
    "    X_holdout, y_holdout = read_features(path_holdout_csv, trigger_type_aux_str)\n",
    "\n",
    "print('train shape:', X_train.shape, y_train.shape)\n",
    "print('holdout shape:', X_holdout.shape, y_holdout.shape)\n",
    "\n",
    "roc, xent = evaluate_classifier(X_train, y_train, X_holdout, y_holdout)\n",
    "print(f'ROC AUC = {roc}')\n",
    "print(f'Cross-Entropy = {xent}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train meta-model using square data and filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using all rows (clean, polygon and instagram filters)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# path_csv = r'confusion-reports/ics_svm/round2-train-dataset/round2-train-dataset_square-25-filters_all-classes_gray.csv'\n",
    "path_csv = r'confusion-reports\\ics_svm\\round3-train-dataset\\round3-train-dataset_square-30-random-filters_all-classes_gray.csv'\n",
    "\n",
    "X_train, y_train = read_features(path_csv, None)\n",
    "# model = svm.SVC(C=11, kernel='rbf', gamma='scale', probability=True)\n",
    "model = LogisticRegression(C=2)\n",
    "model.fit(X_train, y_train)\n",
    "save_obj(model, r'..\\metamodel_07_svm_round3_LR_square35_RANDOM_filters_all-classes.pickle')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using all rows (clean, polygon and instagram filters)\n"
     ]
    }
   ],
   "source": [
    "def create_neural_network():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim=12, activation='relu'))\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def cross_validation(data_x, data_y):\n",
    "    pipeline = Pipeline([('mlp', KerasClassifier(build_fn=create_neural_network, epochs=100, batch_size=32, verbose=0))])\n",
    "    kfold = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=666)\n",
    "    results = cross_val_score(pipeline, X, y, cv=kfold, scoring='neg_log_loss', n_jobs=-1)\n",
    "    results = results[~np.isnan(results)]\n",
    "    results = results[~np.isinf(results)]\n",
    "    print(f'CrossEntropy: mean={-results.mean()} std={results.std()}')\n",
    "\n",
    "def keras_save(model, folder):\n",
    "    if not os.path.isdir(folder):\n",
    "        os.mkdir(folder)\n",
    "    model_json = model.to_json()\n",
    "    with open(os.path.join(folder, 'model.json'), \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(os.path.join(folder, 'model.h5'))\n",
    "\n",
    "def keras_load(folder):\n",
    "    from keras.models import model_from_json\n",
    "\n",
    "    json_file = open(os.path.join(folder, 'model.json'), 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(os.path.join(folder, 'model.h5'))\n",
    "    return loaded_model\n",
    "    \n",
    "def train_nn(data_x, data_y):\n",
    "    model = create_neural_network()\n",
    "    history = model.fit(data_x, data_y, epochs=100, batch_size=32, verbose=0)\n",
    "    \n",
    "    loss, accuracy = model.evaluate(data_x, data_y, verbose=1)\n",
    "    print(f'loss={loss}\\naccuracy={accuracy}')\n",
    "    keras_save(model, r'D:\\Cloud\\MEGA\\TrojAI\\TrojAI-UMD\\metamodel_08_svm_round3_NN-60-30_square30_RANDOM_filters_all-classes')\n",
    "    \n",
    "path_csv = r'confusion-reports\\ics_svm\\round3-train-dataset\\round3-train-dataset_square-30-random-filters_all-classes_gray.csv'\n",
    "X, y = read_features(path_csv, None)\n",
    "\n",
    "# cross_validation(X, y)\n",
    "# train_nn(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MODEL = keras_load(r'..\\metamodel_08_svm_round3_NN-60-30_square30_RANDOM_filters_all-classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'dense_869/bias:0' shape=(1,) dtype=float32, numpy=array([-0.15591685], dtype=float32)>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = MODEL.weights\n",
    "a[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge or simplify datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "def select_columns_from_dataset():\n",
    "    df = pd.read_csv(r'confusion-reports\\ics_svm\\round3-train-dataset\\round3-train-dataset_square-25-filters_all-classes_gray.csv')\n",
    "    for col_to_delete in ['square25_mean_diff', 'square25_std_diff', 'square25_mean', 'square25_std']:\n",
    "        del df[col_to_delete]\n",
    "    df.to_csv(r'confusion-reports\\ics_svm\\round3-train-dataset\\round3-train-dataset_filters_all-classes_gray.csv', index=False)\n",
    "    print('done')\n",
    "\n",
    "def merge_datasets():\n",
    "    for size in [25, 30, 35, 40, 45, 50]:\n",
    "        df1 = pd.read_csv(r'confusion-reports\\ics_svm\\round3-train-dataset\\round3-train-dataset_filters_all-classes_gray.csv')\n",
    "        df2 = pd.read_csv(r'confusion-reports\\ics_svm\\round3-train-dataset\\round3-train-dataset_squares-random-all-classes-gray.csv')\n",
    "        df3 = pd.read_csv(r'confusion-reports\\ics_svm\\round3-train-dataset\\round3-columns.csv')\n",
    "        for col in [f'square{size}_r_mean_diff', f'square{size}_r_std_diff', f'square{size}_r_mean', f'square{size}_r_std']:\n",
    "            df1[col] = df2[col]\n",
    "        df1['trigger_type_aux'] = df3['trigger_type_aux']\n",
    "        new_order_for_columns = [\n",
    "            'model_name', 'model_architecture', 'model_label',\n",
    "            'trigger_type_aux',\n",
    "\n",
    "            f'square{size}_r_mean_diff', f'square{size}_r_std_diff',\n",
    "            'gotham_mean_diff', 'gotham_std_diff',\n",
    "            'kelvin_mean_diff', 'kelvin_std_diff',\n",
    "            'lomo_mean_diff', 'lomo_std_diff',\n",
    "            'nashville_mean_diff', 'nashville_std_diff',\n",
    "            'toaster_mean_diff', 'toaster_std_diff',\n",
    "\n",
    "            'clean_mean', 'clean_std',\n",
    "            f'square{size}_r_mean', f'square{size}_r_std',\n",
    "            'gotham_mean', 'gotham_std',\n",
    "            'kelvin_mean', 'kelvin_std',\n",
    "            'lomo_mean', 'lomo_std',\n",
    "            'nashville_mean', 'nashville_std',\n",
    "            'toaster_mean', 'toaster_std',\n",
    "\n",
    "            'trigger_color', 'num_classes',\n",
    "        ]\n",
    "        df1 = df1[new_order_for_columns]\n",
    "        df1.to_csv(fr'confusion-reports\\ics_svm\\round3-train-dataset\\round3-train-dataset_square-{size}-random-filters_all-classes_gray.csv', index=False) \n",
    "        print('done')\n",
    "merge_datasets()\n",
    "# select_columns_from_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create correct trigger_type_aux column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(r'D:\\Cloud\\MEGA\\TrojAI\\TrojAI-data\\round3-train-dataset\\METADATA.csv')\n",
    "# metadata.columns\n",
    "metadata = metadata[['model_name', 'poisoned', 'model_architecture', 'trigger_type', 'instagram_filter_type', 'polygon_side_count']]\n",
    "metadata['trigger_type_aux'] = [''] * len(metadata)\n",
    "# set(metadata['instagram_filter_type'].to_list())\n",
    "for i in range(len(metadata)):\n",
    "    trigger_type = metadata['trigger_type'].iloc[i]\n",
    "    instagram_filter_type = metadata['instagram_filter_type'].iloc[i].lower().replace('filter','').replace('xform', '')\n",
    "    polygon_side_count = metadata['polygon_side_count'].iloc[i]\n",
    "    if trigger_type == 'None':\n",
    "        metadata['trigger_type_aux'].iloc[i] = 'none'\n",
    "    elif trigger_type == 'instagram':\n",
    "        metadata['trigger_type_aux'].iloc[i] = f'instagram-{instagram_filter_type}'\n",
    "    elif trigger_type == 'polygon':\n",
    "        metadata['trigger_type_aux'].iloc[i] = f'polygon-{polygon_side_count}'\n",
    "metadata.to_csv(rf'D:\\Cloud\\MEGA\\TrojAI\\TrojAI-UMD\\notebooks\\confusion-reports\\ics_svm\\round3-train-dataset\\round3-columns.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
