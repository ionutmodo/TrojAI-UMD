{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from methods import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean: 552\n",
    "polygon: 276\n",
    "kelvin: 63\n",
    "gotham: 51\n",
    "lomo: 60\n",
    "nashville: 55\n",
    "toaster: 47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_features(p_path, p_arch_to_select=None, p_trigger_type=None):\n",
    "def read_features(p_path):\n",
    "    # features for Round 2\n",
    "    column_names_for_features = [\n",
    "        'square_mean_diff', 'square_std_diff',\n",
    "        'gotham_mean_diff', 'gotham_std_diff',\n",
    "        'kelvin_mean_diff', 'kelvin_std_diff',\n",
    "        'lomo_mean_diff', 'lomo_std_diff',\n",
    "        'nashville_mean_diff', 'nashville_std_diff',\n",
    "        'toaster_mean_diff', 'toaster_std_diff'\n",
    "    ]\n",
    "    report = pd.read_csv(p_path)\n",
    "#     if p_arch_to_select is not None:\n",
    "#         report = report[report['model_architecture'] == p_arch_to_select]\n",
    "#     if p_trigger_type is not None:\n",
    "#         indexes = []\n",
    "#         for i in range(len(report)):\n",
    "#             tta = report['trigger_type_aux'].iloc[i].lower()\n",
    "# #             if (p_trigger_type in tta):\n",
    "#             if ('none' in tta):\n",
    "#                 indexes.append(i)\n",
    "#         report = report.iloc[indexes, :]\n",
    "    features = report[column_names_for_features].values\n",
    "    labels = np.array([int(report['model_label'].iloc[i] == 'backdoor') for i in range(len(report))])\n",
    "    return abs(features), labels\n",
    "\n",
    "def evaluate_classifier(train_x, train_y, test_x, test_y):\n",
    "    clf = svm.SVC(kernel='linear', probability=True)\n",
    "    clf.fit(train_x, train_y)\n",
    "    y_score = clf.predict(test_x)\n",
    "    y_pred = clf.predict_proba(test_x)\n",
    "    roc_auc = roc_auc_score(y_true=test_y, y_score=y_score)\n",
    "    cross_entropy = log_loss(y_true=test_y, y_pred=y_pred)\n",
    "    return roc_auc, cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stratified 5-fold validation for square data (apply Round-1 approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold CV (polygon)\n",
      "avg roc =0.6982637182637184\n",
      "std roc =0.028185608275125677\n",
      "\n",
      "avg xent=0.5640477865282053\n",
      "std roc =0.02175045249869255\n"
     ]
    }
   ],
   "source": [
    "trigger_data_to_use = 'polygon'\n",
    "# trigger_data_to_use = 'gotham'\n",
    "# trigger_data_to_use = 'kelvin'\n",
    "# trigger_data_to_use = 'lomo'\n",
    "# trigger_data_to_use = 'nashville'\n",
    "# trigger_data_to_use = 'toaster'\n",
    "\n",
    "dict_trigger_indexes = { 'polygon': 0, 'gotham': 2, 'kelvin': 4, 'lomo': 6, 'nashville': 8, 'toaster': 10 }\n",
    "start_index = dict_trigger_indexes[trigger_data_to_use]\n",
    "\n",
    "path_csv = r'confusion-reports\\ics_svm\\round2-train-dataset\\round2-train-dataset_square20-gotham-kelvin-lomo-nashville-toaster.csv'\n",
    "\n",
    "X, y = read_features(p_path=path_csv) # clean data is automatically added\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "scores_roc, scores_xent = [], []\n",
    "\n",
    "for index, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    X_train, y_train = X[train_index, start_index:start_index+2], y[train_index] # square_diffs have indexes 0,1 !!!\n",
    "    X_test, y_test = X[test_index, start_index:start_index+2], y[test_index] # square_diffs have indexes 0,1 !!!\n",
    "    \n",
    "    roc, xent = evaluate_classifier(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    scores_roc.append(roc)\n",
    "    scores_xent.append(xent)\n",
    "\n",
    "print(f'5-fold CV ({trigger_data_to_use})')\n",
    "print(f'avg roc ={sum(scores_roc) / len(scores_roc)}')\n",
    "print(f'std roc ={np.std(scores_roc)}')\n",
    "print()\n",
    "print(f'avg xent={sum(scores_xent) / len(scores_xent)}')\n",
    "print(f'std roc ={np.std(scores_xent)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stratified 5-fold validation for all data (square + 5 filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg roc =0.8903931203931205\n",
      "std roc =0.01988645636664563\n",
      "\n",
      "avg xent=0.2901835012449866\n",
      "std roc =0.038958773935452023\n"
     ]
    }
   ],
   "source": [
    "path_csv = r'confusion-reports\\ics_svm\\round2-train-dataset\\round2-train-dataset_square20-gotham-kelvin-lomo-nashville-toaster.csv'\n",
    "# dict_trigger_indexes = { 'polygon': 0, 'gotham': 2, 'kelvin': 4, 'lomo': 6, 'nashville': 8, 'toaster': 10 }\n",
    "X, y = read_features(p_path=path_csv) # clean data is automatically added\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "scores_roc, scores_xent = [], []\n",
    "for index, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    X_train, y_train = X[train_index, :], y[train_index]\n",
    "    X_test, y_test = X[test_index, :], y[test_index]\n",
    "#     print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "    roc, xent = evaluate_classifier(X_train, y_train, X_test, y_test)\n",
    "#     print(f'split #{index}: roc={roc:.4f}, xent={xent:.4f}')\n",
    "    scores_roc.append(roc)\n",
    "    scores_xent.append(xent)\n",
    "print(f'avg roc ={sum(scores_roc) / len(scores_roc)}')\n",
    "print(f'std roc ={np.std(scores_roc)}')\n",
    "print()\n",
    "print(f'avg xent={sum(scores_xent) / len(scores_xent)}')\n",
    "print(f'std roc ={np.std(scores_xent)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot (mean, std)  of clean data vs (mean, std) of backdoored data (polygon or instagram filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_csv = r'confusion-reports\\ics_svm\\round2-train-dataset\\round2-train-dataset_square20-gotham-kelvin-lomo-nashville-toaster.csv'\n",
    "dict_trigger_indexes = { 'polygon': 0, 'gotham': 2, 'kelvin': 4, 'lomo': 6, 'nashville': 8, 'toaster': 10 }\n",
    "X_train, y_train = read_features(p_path=path_csv) # clean data is automatically added\n",
    "\n",
    "for trigger_type, idx in dict_trigger_indexes.items():\n",
    "    print(trigger_type, idx)\n",
    "    x_axis = X_train[:, idx]\n",
    "    y_axis = X_train[:, idx+1]\n",
    "    \n",
    "#     list_labels = [['clean', 'backdoored'][y] for y in y_train]\n",
    "#     list_colors = [['#1f77b4', 'orange'][y] for y in y_train]\n",
    "    \n",
    "#     plt.figure(figsize=(10, 6)).patch.set_color('white')\n",
    "#     plt.scatter(x_axis, y_axis, c=list_colors, s=20, cmap=plt.cm.Paired)\n",
    "\n",
    "# #     count_backdoored = 0\n",
    "# #     count_clean = 0\n",
    "# #     for label_int, label_name, color in [(0, 'clean', '#1f77b4'), (1, 'backdoored', 'orange')]:\n",
    "# #         mask = (y_train == label_int)\n",
    "# #         if label_int == 0:\n",
    "# #             count_clean = mask.sum()\n",
    "# #         else:\n",
    "# #             count_backdoored = mask.sum()\n",
    "# #         plt.scatter(x_axis[mask], y_axis[mask], c=color, zorder=10, s=20, cmap=plt.cm.Paired, label=label_name)\n",
    "\n",
    "# #     plt.xlim([0, 20])\n",
    "# #     plt.ylim([0, 6])\n",
    "# #     plt.axvline(x=3, ymin=y_axis.min(), ymax=y_axis.max(), color='k')\n",
    "#     plt.xlabel('mean diff')\n",
    "#     plt.ylabel('std diff')\n",
    "#     plt.title(f'({count_clean} clean models) VS ({count_backdoored} {trigger_type}-trigger models)')\n",
    "#     plt.grid()\n",
    "# #     plt.legend()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## show (size vs ROC) plots for training and holdout for individual architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_square_sizes = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "list_archs = ['densenet121', 'resnet50', 'inceptionv3']\n",
    "\n",
    "for dataset in ['round1-dataset-train', 'round1-holdout-dataset']:\n",
    "    roc_auc_scores = {arch:[] for arch in list_archs}\n",
    "    xent_scores = {arch:[] for arch in list_archs}\n",
    "    for square_size in list_square_sizes:\n",
    "        for arch in list_archs:\n",
    "            path = f'confusion-reports/ics_svm/{dataset}/{dataset}_custom-square-size-{square_size}_backd-original-color_clean-black-color.csv'\n",
    "            X, y = read_features(path, arch)\n",
    "            roc, xent = evaluate_classifier(X, y, X, y)\n",
    "            roc_auc_scores[arch].append(roc)\n",
    "            xent_scores[arch].append(xent)\n",
    "    \n",
    "    print('#############', dataset, '#############')\n",
    "    # PLOT ROC\n",
    "    plt.figure(figsize=(10, 6)).patch.set_color('white')\n",
    "    for arch in roc_auc_scores.keys():\n",
    "        plt.plot(list_square_sizes, roc_auc_scores[arch], 'o-', label=arch)\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.xlabel('square trigger size')\n",
    "    plt.ylabel('ROC AUC for SVM classifier')\n",
    "    plt.xticks(list_square_sizes)\n",
    "    plt.title(f'trigger size vs ROC for {dataset} (architecture-wise)')\n",
    "    plt.show()\n",
    "    \n",
    "    # PLOT XENT\n",
    "    plt.figure(figsize=(10, 6)).patch.set_color('white')\n",
    "    for arch in roc_auc_scores.keys():\n",
    "        plt.plot(list_square_sizes, xent_scores[arch], 'o-', label=arch)\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.xlabel('square trigger size')\n",
    "    plt.ylabel('Cross-Entropy for SVM classifier')\n",
    "    plt.xticks(list_square_sizes)\n",
    "    plt.title(f'trigger size vs Cross-Entropy for {dataset} (architecture-wise)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## show (size vs ROC) plots for training and holdout using all architectures as a whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_square_sizes = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "list_archs = ['densenet121', 'resnet50', 'inceptionv3']\n",
    "\n",
    "for dataset in ['round1-dataset-train', 'round1-holdout-dataset']:\n",
    "    roc_auc_scores = []\n",
    "    xent_scores = []\n",
    "    for square_size in list_square_sizes:\n",
    "        path = f'confusion-reports/ics_svm/{dataset}/{dataset}_custom-square-size-{square_size}_backd-original-color_clean-black-color.csv'\n",
    "        X, y = read_features(path)\n",
    "        \n",
    "        roc, xent = evaluate_classifier(X, y, X, y)\n",
    "        roc_auc_scores.append(roc)\n",
    "        xent_scores.append(xent)\n",
    "        \n",
    "    print('#############', dataset, '#############')\n",
    "    plt.figure(figsize=(10, 6)).patch.set_color('white')\n",
    "    plt.plot(list_square_sizes, roc_auc_scores, 'o-', label='all archs')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.xlabel('square trigger size')\n",
    "    plt.ylabel('ROC AUC for SVM classifier')\n",
    "    plt.xticks(list_square_sizes)\n",
    "    plt.title(f'trigger size vs ROC for {dataset} (all architectures combined)')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6)).patch.set_color('white')\n",
    "    plt.plot(list_square_sizes, xent_scores, 'o-', label='all archs')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.xlabel('square trigger size')\n",
    "    plt.ylabel('Cross-Entropy for SVM classifier')\n",
    "    plt.xticks(list_square_sizes)\n",
    "    plt.title(f'trigger size vs Cross-Entropy for {dataset} (all architectures combined)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train an SVM classifier on training with trigger_size=S and test it on holdout data with trigger_size=S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = pd.DataFrame(columns=['train_size', 'holdout_size', 'roc_train_holdout', 'xent_train_holdout'])\n",
    "n_report = 0\n",
    "for size_train in list_square_sizes:\n",
    "    path_train = f'confusion-reports/ics_svm/round1-dataset-train/round1-dataset-train_custom-square-size-{size_train}_backd-original-color_clean-black-color.csv'\n",
    "    X_train, y_train = read_features(path_train)\n",
    "    \n",
    "    for size_holdout in list_square_sizes:\n",
    "        path_holdout = f'confusion-reports/ics_svm/round1-holdout-dataset/round1-holdout-dataset_custom-square-size-{size_holdout}_backd-original-color_clean-black-color.csv'\n",
    "        X_holdout, y_holdout = read_features(path_holdout)\n",
    "        roc_train_holdout, xent_train_holdout = evaluate_classifier(X_train, y_train, X_holdout, y_holdout)\n",
    "        \n",
    "        report.loc[n_report] = [size_train, size_holdout, roc_train_holdout, xent_train_holdout]\n",
    "        n_report += 1\n",
    "        print(f'size_train={size_train}, size_holdout={size_holdout}: roc={roc_train_holdout}, xent={xent_train_holdout}')\n",
    "report.to_csv('confusion-reports/ics_svm/trained-on-train-tested-on-holdout.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def merge_datasets():\n",
    "#     \"\"\"I need to add the columns toaster_mean_diff, toaster_std_diff, toaster_mean, toaster_std from df2 to df1 and reorder the columns accordingly\"\"\"\n",
    "#     df1 = pd.read_csv(r'confusion-reports\\ics_svm\\round2-train-dataset\\round2-train-dataset_square20-gotham-kelvin-lomo-nashville.csv')\n",
    "#     df2 = pd.read_csv(r'confusion-reports\\ics_svm\\round2-train-dataset\\round2-train-dataset_toaster.csv')\n",
    "#     for col in ['toaster_mean_diff', 'toaster_std_diff', 'toaster_mean', 'toaster_std']:\n",
    "#         df1[col] = df2[col]\n",
    "    \n",
    "#     series_data = []\n",
    "#     for i in range(len(df1)):\n",
    "#         trigger_type = df1.iloc[i]['trigger_type']\n",
    "#         trigger_type_option = df1.iloc[i]['trigger_type_option']\n",
    "#         if trigger_type == 'instagram':\n",
    "#             series_data.append(trigger_type_option.replace('XForm', '').replace('Filter', ''))\n",
    "#         else:\n",
    "#             if trigger_type == 'None':\n",
    "#                 series_data.append(trigger_type)\n",
    "#             else:\n",
    "#                 series_data.append(f'{trigger_type}-{trigger_type_option}')\n",
    "#     df1['trigger_type_aux'] = series_data\n",
    "    \n",
    "#     new_order_for_columns = [\n",
    "#         'model_name', 'model_architecture', 'model_label',\n",
    "#         'trigger_type_aux', # 'trigger_type', 'trigger_type_option',\n",
    "\n",
    "#         'square_mean_diff', 'square_std_diff',\n",
    "#         'gotham_mean_diff', 'gotham_std_diff',\n",
    "#         'kelvin_mean_diff', 'kelvin_std_diff',\n",
    "#         'lomo_mean_diff', 'lomo_std_diff',\n",
    "#         'nashville_mean_diff', 'nashville_std_diff',\n",
    "#         'toaster_mean_diff', 'toaster_std_diff',\n",
    "\n",
    "#         'clean_mean', 'clean_std',\n",
    "#         'square_mean', 'square_std',\n",
    "#         'gotham_mean', 'gotham_std',\n",
    "#         'kelvin_mean', 'kelvin_std',\n",
    "#         'lomo_mean', 'lomo_std',\n",
    "#         'nashville_mean', 'nashville_std',\n",
    "#         'toaster_mean', 'toaster_std',\n",
    "\n",
    "#         'trigger_color', 'num_classes',\n",
    "#     ]\n",
    "#     df1 = df1[new_order_for_columns]\n",
    "#     df1.to_csv(r'confusion-reports\\ics_svm\\round2-train-dataset\\round2-train-dataset_square20-gotham-kelvin-lomo-nashville-toaster.csv', index=False)\n",
    "#     print('done')\n",
    "# # merge_datasets()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
